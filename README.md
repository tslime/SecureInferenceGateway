# ğŸ” Secure Multi-Model Inference Gateway (FastAPI + Ollama)

> A lightweight, secure, and extensible HTTP gateway for dispatching prompt requests to local LLMs via Ollama.  
> Designed for air-gapped, privacy-sensitive environments (e.g., defense, aerospace, R&D).

---

## ğŸ§  Motivation

In high-security environments (like defense, enterprise research, or robotics), cloud-based LLMs can't be used due to privacy concerns. This gateway allows:

- Secure **local inference** using multiple models
- Controlled access with API key authentication
- Clean routing between models via a RESTful interface
- Future extensions like **multi-model voting**, **access control**, and **auditing**

---

## âœ… Core Features

- ğŸ” **POST /request** â€” send prompts and route to correct model
- ğŸ” **API Key Auth** â€” secure access with `X-API-KEY`
- ğŸ§  **Model Validation** â€” supports any installed Ollama model
- ğŸ§¼ **Streaming Response Handling** â€” parses Ollama JSONL output
- ğŸ”§ **Graceful Error Handling** â€” handles connection and inference failures
- ğŸ›¡ï¸ **Air-Gapped Ready** â€” no external dependencies

---

## ğŸ” Security Features

| Feature | Description |
|--------|-------------|
| ğŸ”‘ API Key Authentication | Blocks unauthorized access (`X-API-KEY: admin`) |
| ğŸš« Model Filtering | Rejects prompts for unknown models |
| ğŸ§¯ Ollama Error Handling | Returns `502` if Ollama is unreachable |
| âœ… Response Sanitization | Only returns plain text content |
| ğŸŒ No External Calls | All inference runs **entirely locally**

---

## ğŸš€ API Usage

### ğŸ” POST `/request`

```json
{
  "model": "phi",
  "request": "Give me a fun fact about black holes."
}
