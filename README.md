# Secure Multi-Model Inference Gateway (FastAPI + Ollama)

## Overview

Secure Inference Gateway is a Python-based FastAPI gateway that acts as a secure interface to serve machine learning models via HTTP. It integrates with an Ollama server for model inference and enforces API key-based authentication on requests, making it suitable for secure deployment in internal networks.

## Purpose

- Provide a secure, authenticated HTTP API for accessing and generating responses from locally hosted machine learning models.
- Act as a gateway between clients and an Ollama server running on the local network.

## Main Features

- **FastAPI Web Server**: Exposes RESTful endpoints for model inference.
- **API Key Authentication**: Requires `X-API-KEY` header for authorization (default key: `admin`).
- **Dynamic Model Discovery**: Fetches available models from the Ollama server.
- **Streaming Response Handling**: Handles streamed model responses efficiently.
- **Customizable Host/Port**: Easily configurable for different network setups.

## Architecture

- **Ollama Server**: The backend ML inference engine, expected to be running at `http://127.0.0.1:11434`.
- **FastAPI Gateway**: Receives and forwards requests to Ollama, enforcing authentication and formatting responses.

## Dependencies

### 1. Python Dependencies

Install the required Python packages:

```bash
pip install fastapi uvicorn requests pydantic
```

### 2. Install Ollama

Ollama is required to serve models locally.

- Visit [Ollama's official website](https://ollama.com/download) for installation instructions.
- Typical installation (Linux/macOS):

    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ```

- For Windows, download the installer from their website.

- After installation, start Ollama from a different terminal:

    ```bash
    ollama serve
    ```

### 3. Pulling Models with Ollama

Ollama supports various models such as Llama 2, Mistral, etc.

To pull a model (example):

```bash
ollama pull llama2 tinyllama phi gemma 
```

You can list available models with:

```bash
ollama list
```

To pull other models, see [Ollama's model library](https://ollama.com/library).

### 4. Run Secure Inference Gateway

Make sure your Ollama server is running locally (`ollama serve`) and models are pulled.

Execute the main server script:

```bash
python Secureig.py
```

By default, it launches on `localhost:8085`. Change the IP and port in the script as needed.

## Usage

### API Authentication

Clients must supply an API key:
```
X-API-KEY: admin
```
Requests without this key will receive a `403 Unauthorized access` response.

### API Endpoints

#### `POST /request`

- **Purpose**: Submit a prompt to a selected model.
- **Request Body** (`RStructure`):
  - `model`: Name of the model (as discovered via `/api/tags` from Ollama).
  - `request`: The prompt/content for inference.

- **Headers**:
  - `X-API-KEY`: API key (must be "admin" by default or provide ones that you generate to clients).

- **Response**:
  - On success: Text response generated by the selected model.
  - On error: Plain text error message, with appropriate HTTP status codes.

#### Model Discovery

Internally, the gateway queries Ollama’s `/api/tags` endpoint to fetch available model names. This is achieved using the following algorithm:

```
def get_models():

    try:
        mod = requests.get("http://127.0.0.1:11434/api/tags")

        temp = []
        for m in mod.json()["models"]:
            temp.append(m["name"].split(":")[0])
    
        return set(temp)
    except requests.exceptions.RequestException as e:
        print("There are no models \n",e)
        return PlainTextResponse("There are no models \n",status_code=403)

```

As delineated in the algorithm, ollama keeps the names of the models in the /api/tags directory. Note that many model names come with an additional tag following a colon, which usually indicates the version of the model (e.g., llama:latest). These secondary tags are removed using `split(":")[0]' to keep only the models' names together in a set. This resulting set is useful to validate the presence of a given model when a client sends a request to prompt a specific model. In this way, we can inform the client whether we can complete its request or not. For example, by replying to a client with the message "Unsupported model" in case the model doesn't exist in our database (see the post request code for more details). 

## Example Usage

```bash
curl -X POST http://localhost:8085/request \
  -H "X-API-KEY: admin" \
  -H "Content-Type: application/json" \
  -d '{"model": "llama2", "request": "What do you think about gravity?"}'
```

## Error Handling

- **No Models Available**: Returns 403 and message if Ollama server is unreachable or has no models.
- **Unauthorized Access**: Returns 403 if API key is missing or incorrect.
- **Model Unavailable**: Returns 403 if selected model does not exist or cannot be served.

## Mission-Critical & Defense Applications

SecureInferenceGateway is especially suited for high-security, mission-critical environments such as defense departments, intelligence agencies, and critical infrastructure operations. Here’s why:

### Key Advantages

- **Network Isolation:** Can be deployed in strictly controlled internal networks, minimizing exposure to external threats.
- **Local Model Hosting:** Integrates with Ollama to ensure all AI models and data remain on-premises, supporting strict data sovereignty requirements.
- **Authentication & Access Control:** API key-based access (extensible to advanced mechanisms) helps restrict use to authorized personnel only.
- **Auditability:** Centralized gateway for all inference requests enables robust logging and activity monitoring.
- **Customizability:** Built on FastAPI, allowing integration of additional security features like multi-factor authentication, role-based access, and advanced encryption.

### Example Use Cases

- **Battlefield Decision Support:** Secure, real-time access to AI models for operational analysis and tactical assistance.
- **Mission Planning:** Allows analysts and planners to query LLMs for logistics, intelligence, and scenario simulation in secure environments.
- **Research & Development:** Supports sensitive AI research where data and models must remain confidential and access must be tightly controlled.

### Recommendations for Defense/Mission-Critical Use

- **Upgrade Authentication:** Implement stronger authentication (certificates, MFA, etc.) beyond simple API keys.
- **Enforce Encryption:** Use HTTPS/TLS for all traffic.
- **Enhance Monitoring:** Integrate with SIEM and automated alerting systems.
- **Comply with Standards:** Ensure all components conform to relevant security, compliance, and operational standards.

> **Note:** While Secure Inference Gateway provides a strong foundation for secure AI inference, organizations deploying it for defense or mission-critical purposes should perform thorough security audits and adapt the codebase to meet their specific requirements.


