# Secure Multi-Model Inference Gateway (FastAPI + Ollama)

## Overview

Secure Inference Gateway is a Python-based FastAPI gateway that acts as a secure interface to serve machine learning models via HTTP. It integrates with an Ollama server for model inference and enforces API key-based authentication on requests, making it suitable for secure deployment in internal networks.

## Purpose

- Provide a secure, authenticated HTTP API for accessing and generating responses from locally hosted machine learning models.
- Act as a gateway between clients and an Ollama server running on the local network.

## Main Features

- **FastAPI Web Server**: Exposes RESTful endpoints for model inference.
- **API Key Authentication**: Requires `X-API-KEY` header for authorization (default key: `admin`).
- **Dynamic Model Discovery**: Fetches available models from the Ollama server.
- **Streaming Response Handling**: Handles streamed model responses efficiently.
- **Customizable Host/Port**: Easily configurable for different network setups.

## Architecture

- **Ollama Server**: The backend ML inference engine, expected to be running at `http://127.0.0.1:11434`.
- **FastAPI Gateway**: Receives and forwards requests to Ollama, enforcing authentication and formatting responses.

## Dependencies

### 1. Python Dependencies

Install the required Python packages:

```bash
pip install fastapi uvicorn requests pydantic
```

### 2. Install Ollama

Ollama is required to serve models locally.

- Visit [Ollama's official website](https://ollama.com/download) for installation instructions.
- Typical installation (Linux/macOS):

    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ```

- For Windows, download the installer from their website.

- After installation, start Ollama from a different terminal:

    ```bash
    ollama serve
    ```

### 3. Pulling Models with Ollama

Ollama supports various models such as Llama 2, Mistral, etc.

To pull a model (example):

```bash
ollama pull llama2 tinyllama phi gemma 
```

You can list available models with:

```bash
ollama list
```

To pull other models, see [Ollama's model library](https://ollama.com/library).

### 4. Run Secure Inference Gateway

Make sure your Ollama server is running locally (`ollama serve`) and models are pulled.

Execute the main server script:

```bash
python Secureig.py
```

By default, it launches on `localhost:8085`. Change the IP and port in the script as needed.

## Usage

### API Authentication

Clients must supply an API key:
```
X-API-KEY: admin
```
Requests without this key will receive a `403 Unauthorized access` response.

### API Endpoints

#### `POST /request`

- **Purpose**: Submit a prompt to a selected model.
- **Request Body** (`RStructure`):
  - `model`: Name of the model (as discovered via `/api/tags` from Ollama).
  - `request`: The prompt/content for inference.

- **Headers**:
  - `X-API-KEY`: API key (must be "admin" by default or provide ones that you generate to clients).

- **Response**:
  - On success: Text response generated by the selected model.
  - On error: Plain text error message, with appropriate HTTP status codes.

#### Model Discovery

Internally, the gateway queries Ollamaâ€™s `/api/tags` endpoint to fetch available model names.

## Example Usage

```bash
curl -X POST http://192.168.2.57:8085/request \
  -H "X-API-KEY: admin" \
  -H "Content-Type: application/json" \
  -d '{"model": "llama2", "request": "What do you think about gravity?"}'
```

## Error Handling

- **No Models Available**: Returns 403 and message if Ollama server is unreachable or has no models.
- **Unauthorized Access**: Returns 403 if API key is missing or incorrect.
- **Model Unavailable**: Returns 403 if selected model does not exist or cannot be served.
## Contribution

For contributions, open issues or pull requests in the [GitHub repository](https://github.com/tslime/SecureInferenceGateway).

